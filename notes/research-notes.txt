January 6 notes
    # TODO data exploration
        - Look at class distribution
            - Most of the comment in train have 0 toxicity label
            - Some comment can have all 6 labels
        - Look at importance words for every classes
            WordCloud.ipynb
            Seems like people like to type some word 2 times (KILL KILL, NIGGER NIGGER, ...)
    # TODO feature exploration
        LSTM feature
            Look at how much feature we want to keep AKA most common words
            Look at max length of a sequence

    # IDEA Could do first a binary classification if comment have label and then predict every label
    # IDEA about standard feature
        - Presence of popular word in a classe that are not popular overall
        - Presence of a WIKI bracket [ WIKI something]
        - Presence of Markdown style
            I guess people that comment rude things won't take time to format a message?.. Even if people can be crazy

    # Note Regarding the corpus
        Contain links
        Contain machine understanding character (ex. [WIKI_LINK: Wikipedia:How to edit a page@edit a page])
            # TODO remove that from training data? Not clear it's gonna help tho

    It's interesting to see that Wikipedia comments are kind of a communities. People meet in real life and chat on IRC.
    Trace of these things are left in some comments.

    Next steps:
        - Create a first Keras LSTM model based on words. I'm curious to see if it can do the job...
            0.051 which is about half the leaderboard. Seems interesting...
            It was a bidirectionnal

    # TODO choose a sequence length
    # TODO choose a number of feature
        Should look at plot, same as length
        Should also take care of populare word in category

    By reading this paper https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf,
    I'm thinking I should try some other baseline such as

    # Note Regarding W2V
        I'm using fasttext and a zero vector if the word doesn't exist there...
        Maybe I should use a random vector and let the embedding layer do its job?
        Not sure of the effect...

    # Note embeding layer
        Should we train the mebeding layer or not? Not clear for me what would be best. Thinking about a paper about that
        Paper could test with different benchmark dataset the effect difference between 4 things:
            - Fast text as base, no train
            - Fast text as base with train. Unkown as 0
            - Fast text with online training for unknown
            - Embeding from random init

